import os
import sys
import traceback
import gradio as gr
from dotenv import load_dotenv

# .env yÃ¼kle
load_dotenv()

# core modÃ¼lÃ¼nden importlar
from core import (
    DATA_DIR,
    PERSIST_DIR,
    ensure_dirs,
    build_or_update_vectorstore,
    load_documents,
    split_documents,
    get_retriever,
    get_llm,
    safe_compose_context,
    context_is_relevant,
    reduce_repetition,
    build_prompt,
    format_sources_by_document,
    fetch_weather_summary,
)


def clean_llm_output(text: str) -> str:
    """
    LLM Ã§Ä±ktÄ±sÄ±nÄ± temizler: Prompt talimatlarÄ±nÄ±, kural aÃ§Ä±klamalarÄ±nÄ± vs. kaldÄ±rÄ±r.
    """
    import re
    
    if not text or len(text) < 5:
        return text
    
    # Prompt talimatlarÄ±nÄ± ve kurallarÄ± kaldÄ±r
    lines = text.split('\n')
    cleaned_lines = []
    
    skip_patterns = [
        r'KRÄ°TÄ°K KURALLAR',
        r'Ã–NEMLÄ°:',
        r'BaÄŸlamda.*?Ã§Ä±karamÄ±yorum',
        r'Sadece baÄŸlamda.*?kullan',
        r'varsayÄ±m yapma',
        r'Ã–rnek:',
        r'^\d+\.\s+',  # 1. 2. gibi numaralÄ± liste
    ]
    
    for line in lines:
        line_lower = line.lower()
        should_skip = False
        
        for pattern in skip_patterns:
            if re.search(pattern, line_lower):
                should_skip = True
                break
        
        if not should_skip and line.strip():
            cleaned_lines.append(line)
    
    cleaned = '\n'.join(cleaned_lines)
    
    # Fazla boÅŸluklarÄ± temizle
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    cleaned = re.sub(r' +', ' ', cleaned)
    cleaned = cleaned.strip()
    
    # EÄŸer Ã§ok kÄ±sa kaldÄ±ysa orijinal metni dÃ¶ndÃ¼r
    if len(cleaned) < 10 and len(text) > 50:
        return text.strip()
    
    return cleaned if cleaned else text.strip()


def compose_memory_text(history_state: list[tuple[str, str]]) -> str:
    """
    Ã–nceki konuÅŸmalarÄ± kÄ±sa bir hafÄ±za metnine Ã§evirir.
    """
    if not history_state:
        return ""

    entries = history_state[-6:]
    lines = []
    for user_msg, bot_msg in entries:
        lines.append(f"KullanÄ±cÄ±: {user_msg}")
        lines.append(f"Bot: {bot_msg}")

    return "\n".join(lines)


def reset_data_store() -> str:
    import shutil

    paths = [PERSIST_DIR, DATA_DIR]
    for path in paths:
        if os.path.exists(path):
            shutil.rmtree(path, ignore_errors=True)

    ensure_dirs()
    return "Veri klasÃ¶rleri temizlendi (chroma_db + data)."


def ui_answer(
    message,
    history_state,
    k_value,
    use_mmr,
    prompt_format,
    use_turkish_embedding,
    weather_city,
    use_weather_api,
):
    """
    Sohbet Fonksiyonu
    Input: Mesaj ve Gizli GeÃ§miÅŸ (history_state)
    Output: (BoÅŸ mesaj kutusu, Chatbot gÃ¶rÃ¼nÃ¼mÃ¼, GÃ¼ncel Gizli GeÃ§miÅŸ)
    """
    if history_state is None:
        history_state = []

    try:
        # Query expansion: EÄŸitim sorularÄ± iÃ§in daha spesifik kelimeler ekle
        expanded_query = message
        query_lower = message.lower()
        # EÄŸitim sorularÄ± iÃ§in CV'nin eÄŸitim bÃ¶lÃ¼mÃ¼nÃ¼ bulmak iÃ§in daha spesifik kelimeler
        if any(kw in query_lower for kw in ["eÄŸitim", "okul", "Ã¼niversite", "mezun", "bÃ¶lÃ¼m", "cv", "Ã¶zgeÃ§miÅŸ"]):
            # EÄŸitim sorularÄ±nda sadece eÄŸitimle ilgili kelimeler kullan, proje kelimelerini kullanma
            expanded_query = message + " Ã¼niversite okul mezun bÃ¶lÃ¼m fakÃ¼lte eÄŸitim Ã¶ÄŸrenci lisans yÃ¼ksek"
        elif any(kw in query_lower for kw in ["beceri", "yetenek", "programlama", "dil"]):
            expanded_query = message + " beceri yetenek programlama dil teknoloji araÃ§"
        
        # RAG Ä°ÅŸlemleri
        retriever = get_retriever(
            PERSIST_DIR, 
            k=int(k_value), 
            use_mmr=use_mmr,
            turkish_focused=use_turkish_embedding
        )
        docs = retriever.invoke(expanded_query)
        context = safe_compose_context(docs)

        # Ä°steÄŸe baÄŸlÄ±: Hava durumu API Ã¶zeti
        api_summary = ""
        if use_weather_api:
            try:
                api_text, is_real = fetch_weather_summary(weather_city)
                api_summary = api_text
            except Exception as api_err:
                print(f"Hava durumu API hatasÄ± (UI): {api_err}")
        
        # Minimal debug (sadece hata durumlarÄ±nda)
        if not context or not context_is_relevant(message, context):
            print(f"âš ï¸ Soru: '{message[:50]}...' - Context yetersiz veya ilgisiz")

        # Memory metnini hazÄ±rla
        memory_text = compose_memory_text(history_state)

        # Belge baÄŸlamÄ± + hafÄ±za + API Ã¶zeti tek promptta birleÅŸsin
        parts = []
        if memory_text:
            parts.append(f"Ã–nceki konuÅŸmalar:\n{memory_text}")
        if context:
            parts.append(f"Belge baÄŸlamÄ±:\n{context}")
        if api_summary:
            parts.append(f"API (hava durumu) Ã¶zeti:\n{api_summary}")

        full_context = "\n\n".join(parts).strip()

        llm = get_llm()
        prompt = build_prompt(message, full_context, prompt_format, sources=docs)
        
        if not full_context or not context_is_relevant(message, full_context):
            answer = "Bu belgeden Ã§Ä±karamÄ±yorum."
            sources_text = ""
        else:
            answer = llm.invoke(prompt).strip()
            answer = reduce_repetition(answer)
            
            # LLM Ã§Ä±ktÄ±sÄ±nÄ± temizle: Prompt talimatlarÄ±nÄ±, kural aÃ§Ä±klamalarÄ±nÄ± vs. kaldÄ±r
            answer = clean_llm_output(answer)
            
            # KaynaklarÄ± belge bazÄ±nda formatla
            sources_text = format_sources_by_document(docs)

        # KaynaklarÄ± ekle (belge bazÄ±nda gruplanmÄ±ÅŸ)
        if sources_text:
            full_response = answer + f"\n\nğŸ“š Kaynaklar:\n{sources_text}"
        else:
            full_response = answer

        # GeÃ§miÅŸe ekle (User, Bot)
        history_state.append((message, full_response))
        
        # 1. Msg kutusunu temizle ("")
        # 2. Chatbot'a geÃ§miÅŸi ver (history_state)
        # 3. State'i gÃ¼ncelle (history_state)
        return "", history_state, history_state

    except Exception as e:
        print(f"Hata: {e}")
        traceback.print_exc()
        error_msg = f"Hata oluÅŸtu: {str(e)}"
        history_state.append((message, error_msg))
        return "", history_state, history_state


def ui_upload(files):
    """
    Dosya yÃ¼kleme fonksiyonu
    """
    print("--- Dosya YÃ¼kleme Ä°steÄŸi Geldi ---")
    
    # 422 HatasÄ± iÃ§in gÃ¼venlik kontrolÃ¼
    if files is None:
        return "âš ï¸ Dosya seÃ§ilmedi."
        
    # Gradio versiyonuna gÃ¶re files listesi veya tek obje gelebilir
    file_paths = []
    
    # Liste mi deÄŸil mi kontrol et
    if isinstance(files, list):
        for f in files:
            # Gradio dosyayÄ± bir nesne olarak mÄ± yoksa string yol olarak mÄ± gÃ¶nderiyor?
            if isinstance(f, str):
                file_paths.append(f)
            elif hasattr(f, 'name'): # Gradio temp file objesi
                file_paths.append(f.name)
    elif isinstance(files, str):
        file_paths.append(files)
    elif hasattr(files, 'name'):
        file_paths.append(files.name)

    if not file_paths:
        return "âš ï¸ Dosya yolu okunamadÄ±."

    print(f"Ä°ÅŸlenecek dosyalar: {file_paths}")
    
    saved_paths = []
    ensure_dirs()

    try:
        # DosyalarÄ± kopyala
        import shutil
        for src_path in file_paths:
            filename = os.path.basename(src_path)
            target_path = os.path.join(DATA_DIR, filename)
            shutil.copy2(src_path, target_path)
            saved_paths.append(target_path)

        # Ä°ndeksle
        docs = load_documents(saved_paths)
        if not docs:
            return "âŒ Metin okunamadÄ±."
            
        chunks = split_documents(docs)
        build_or_update_vectorstore(chunks, persist_directory=PERSIST_DIR)
        
        return f"âœ… BaÅŸarÄ±lÄ±! {len(chunks)} parÃ§a eklendi."

    except Exception as e:
        print(f"Upload HatasÄ±: {e}")
        traceback.print_exc()
        return f"âŒ Hata: {str(e)}"


def build_demo():
    ensure_dirs()
    
    # CSS: Chatbot yÃ¼ksekliÄŸi
    css = "#chatbot {height: 500px !important; overflow: auto;}"

    with gr.Blocks(title="RAG AsistanÄ±", css=css) as demo:
        # --- STATE (Gizli HafÄ±za) ---
        # Chatbot'u input olarak kullanmak yerine bunu kullanacaÄŸÄ±z
        history_state = gr.State([]) 

        gr.Markdown("## ğŸ“„ Belge Soru-Cevap Sistemi")

        with gr.Row():
            with gr.Column(scale=1):
                file_uploader = gr.File(
                    label="PDF/TXT YÃ¼kle", 
                    file_types=[".pdf", ".txt"], 
                    file_count="multiple",
                    type="filepath" # Bu ayar Ã¶nemli
                )
                upload_btn = gr.Button("Ä°ndeksi GÃ¼ncelle", variant="primary")
                upload_info = gr.Textbox(label="Durum", interactive=False)
                data_reset_status = gr.Textbox(label="Veri Durumu", interactive=False)
                clear_data_btn = gr.Button("Veri Temizle (chroma_db + data)", variant="secondary")
            
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="Sohbet", elem_id="chatbot")
                with gr.Row():
                    msg = gr.Textbox(
                        label="Sorunuz", 
                        placeholder="YazÄ±n ve Enter'a basÄ±n...",
                        scale=4
                    )
                    clear = gr.Button("Temizle", scale=1)

        with gr.Accordion("Ayarlar", open=False):
            k_slider = gr.Slider(
                minimum=1,
                maximum=15,
                value=6,
                step=1,
                label="k (CV sorularÄ± iÃ§in 8-10 Ã¶nerilir)",
            )
            mmr_checkbox = gr.Checkbox(value=True, label="MMR")
            prompt_format = gr.Dropdown(
                ["kÄ±sa", "madde", "Ã¶zet_madde", "Ã¶nce_sonuÃ§"],
                value="kÄ±sa",
                label="Format",
            )
            turkish_embedding = gr.Checkbox(
                value=False,
                label="TR Embedding",
            )

            gr.Markdown("### ğŸŒ¤ Hava Durumu API AyarlarÄ±")
            weather_city = gr.Textbox(
                label="Åehir (hava durumu iÃ§in)",
                placeholder="Ã–rn: Ä°stanbul",
                value="Ä°stanbul",
            )
            use_weather_api = gr.Checkbox(
                value=False,
                label="Hava durumu API'sini kullan (OPENWEATHER_API_KEY yoksa MOCK veri dÃ¶ner)",
            )

        # --- OLAYLAR (EVENTS) ---
        
        # 1. Upload Butonu
        upload_btn.click(
            fn=ui_upload, 
            inputs=[file_uploader], 
            outputs=[upload_info]
        )
        clear_data_btn.click(fn=reset_data_store, inputs=None, outputs=[data_reset_status])

        # 2. Mesaj GÃ¶nderme
        # DÄ°KKAT: inputs iÃ§inde 'chatbot' YOK. 'history_state' VAR.
        msg.submit(
            fn=ui_answer,
            inputs=[
                msg,
                history_state,
                k_slider,
                mmr_checkbox,
                prompt_format,
                turkish_embedding,
                weather_city,
                use_weather_api,
            ],
            outputs=[msg, chatbot, history_state],  # Ã‡Ä±ktÄ± sÄ±rasÄ±: MesajKutusu, GÃ¶rselChat, GizliState
        )

        # 3. Temizle
        def clear_history():
            return [], [] # Hem chatbot hem state temizlenir
            
        clear.click(fn=clear_history, inputs=None, outputs=[chatbot, history_state])

    return demo

def _read_server_port() -> int:
    raw = os.getenv("SERVER_PORT", "7860")
    try:
        return int(raw)
    except ValueError:
        print(f"âš ï¸ SERVER_PORT deÄŸeri geÃ§ersiz: {raw}. VarsayÄ±lan 7860 kullanÄ±lacak.")
        return 7860


if __name__ == "__main__":
    base_port = _read_server_port()
    
    demo = build_demo()
    
    # Port Ã§akÄ±ÅŸmasÄ± durumunda alternatif portlarÄ± dene
    for port_offset in range(10):
        try:
            port = base_port + port_offset
            print(f"BaÅŸlatÄ±lÄ±yor... Port: {port}")
            demo.launch(server_port=port, share=False)
            break
        except OSError as e:
            if "Cannot find empty port" in str(e) or "Port" in str(e):
                print(f"Port {port} kullanÄ±mda, {port + 1} deniyor...")
                continue
            else:
                raise
    else:
        print(f"âŒ 7860-7869 arasÄ± tÃ¼m portlar kullanÄ±mda!")
        print("LÃ¼tfen bir Python iÅŸlemini durdurun veya SERVER_PORT deÄŸiÅŸkenini ayarlayÄ±n.")