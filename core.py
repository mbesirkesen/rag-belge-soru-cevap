import os
from typing import Iterable, List, Tuple

from langchain_community.document_loaders import PDFMinerLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline


SUPPORTED_EXTENSIONS = {".txt", ".pdf"}
PERSIST_DIR = os.path.join(os.path.dirname(__file__), "chroma_db")
DATA_DIR = os.path.join(os.path.dirname(__file__), "data")


def _load_from_txt(path: str) -> List[Document]:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        text = f.read()
    # Keep file reference in metadata for source display
    return [Document(page_content=text, metadata={"source": os.path.relpath(path), "type": "txt"})]


def _load_from_pdf(path: str) -> List[Document]:
    """
    PDF metnini daha temiz Ã§Ä±karmak iÃ§in PDFMiner kullan.
    (PyPDF bazÄ± dosyalarda CID/Unicode bozulmalarÄ± Ã¼retebiliyor)
    """
    loader = PDFMinerLoader(path)
    docs = loader.load()
    for d in docs:
        d.metadata["source"] = os.path.relpath(path)
    return docs


def load_documents(paths: Iterable[str]) -> List[Document]:
    documents: List[Document] = []
    for path in paths:
        ext = os.path.splitext(path)[1].lower()
        if ext == ".txt":
            documents.extend(_load_from_txt(path))
        elif ext == ".pdf":
            documents.extend(_load_from_pdf(path))
        else:
            continue
    return documents


def split_documents(documents: List[Document], chunk_size: int = 800, chunk_overlap: int = 120) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        add_start_index=True,
    )
    return splitter.split_documents(documents)


def get_embeddings(turkish_focused: bool = False):
    """
    Embedding modeli dÃ¶ndÃ¼rÃ¼r.
    
    Args:
        turkish_focused: True ise TÃ¼rkÃ§e odaklÄ± Ã§ok dilli model kullanÄ±r
    """
    if turkish_focused:
        # TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ Ã§ok dilli model
        return HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    # Small, CPU-friendly model (varsayÄ±lan)
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


def build_or_update_vectorstore(chunks: List[Document], persist_directory: str = PERSIST_DIR, turkish_focused: bool = False) -> Chroma:
    os.makedirs(persist_directory, exist_ok=True)
    embeddings = get_embeddings(turkish_focused=turkish_focused)
    
    # If a store exists, add to it; otherwise create new
    # ChromaDB'nin varlÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in daha gÃ¼venli yÃ¶ntem
    db_exists = False
    try:
        existing_files = os.listdir(persist_directory)
        # ChromaDB en azÄ±ndan birkaÃ§ dosya oluÅŸturur
        db_exists = len(existing_files) > 0 and any(f.endswith('.sqlite3') or f == 'chroma.sqlite3' for f in existing_files)
    except:
        db_exists = False
    
    if db_exists:
        try:
            vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
            vectorstore.add_documents(chunks)
            # langchain-chroma 0.1+: otomatik kalÄ±cÄ±; persist() yok
            return vectorstore
        except Exception as e:
            print(f"Varolan indekse ekleme hatasÄ±: {e}. Yeni indeks oluÅŸturuluyor...")
            # Hata varsa yeni oluÅŸtur
            return Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)
    else:
        return Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)


def get_retriever(persist_directory: str = PERSIST_DIR, k: int = 6, use_mmr: bool = True, turkish_focused: bool = False):
    """
    Retriever dÃ¶ndÃ¼rÃ¼r.
    
    Args:
        persist_directory: ChromaDB dizin yolu
        k: DÃ¶ndÃ¼rÃ¼lecek dokÃ¼man sayÄ±sÄ± (3-6 arasÄ± Ã¶nerilir)
        use_mmr: True ise Maximum Marginal Relevance kullanÄ±r (Ã§eÅŸitlilik iÃ§in)
        turkish_focused: True ise TÃ¼rkÃ§e odaklÄ± embedding modeli kullanÄ±r
    """
    embeddings = get_embeddings(turkish_focused=turkish_focused)
    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    
    if use_mmr:
        # MMR ile Ã§eÅŸitlilik
        fetch_k = max(k * 3, 20)  # fetch_k en az k*3 veya 20
        return vectorstore.as_retriever(
            search_type="mmr", 
            search_kwargs={"k": k, "fetch_k": fetch_k, "lambda_mult": 0.5}
        )
    else:
        # Basit similarity search
        return vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )


def discover_data_files(root_dir: str = DATA_DIR) -> List[str]:
    os.makedirs(root_dir, exist_ok=True)
    file_paths: List[str] = []
    for dirpath, _, filenames in os.walk(root_dir):
        for name in filenames:
            ext = os.path.splitext(name)[1].lower()
            if ext in SUPPORTED_EXTENSIONS:
                file_paths.append(os.path.join(dirpath, name))
    return file_paths


def format_source_list(docs: List[Document]) -> List[Tuple[str, str]]:
    """
    Returns list of tuples (source, location) e.g. (my.pdf, "page 3") or (a.txt, "chunk at 1234")
    """
    items: List[Tuple[str, str]] = []
    for d in docs:
        source = d.metadata.get("source") or d.metadata.get("file_path") or "unknown"
        page = d.metadata.get("page")
        start_index = d.metadata.get("start_index")
        location = f"page {page}" if page is not None else f"offset {start_index}" if start_index is not None else ""
        items.append((source, location))
    return items


def format_sources_by_document(docs: List[Document]) -> str:
    """
    KaynaklarÄ± belge bazÄ±nda gruplayarak formatlar.
    Her belge iÃ§in sayfa bilgilerini toplar ve ayrÄ± satÄ±rlarda gÃ¶sterir.
    
    Returns:
        FormatlanmÄ±ÅŸ kaynak string'i
    """
    from collections import defaultdict
    
    # Belge bazÄ±nda grupla: {source: [pages]}
    doc_pages = defaultdict(set)
    for d in docs:
        source = d.metadata.get("source") or d.metadata.get("file_path") or "unknown"
        # Sadece dosya adÄ±nÄ± al (tam yol yerine)
        filename = os.path.basename(source)
        
        page = d.metadata.get("page")
        if page is not None:
            doc_pages[filename].add(page)
        else:
            # Sayfa yoksa boÅŸ set ile iÅŸaretle
            if filename not in doc_pages:
                doc_pages[filename] = set()
    
    # Formatla: Her belge iÃ§in sayfalarÄ± sÄ±rala
    lines = []
    for filename, pages in sorted(doc_pages.items()):
        if pages:
            sorted_pages = sorted(pages)
            if len(sorted_pages) == 1:
                lines.append(f"ğŸ“„ {filename} (sayfa {sorted_pages[0]})")
            else:
                # Birden fazla sayfa varsa aralÄ±k gÃ¶ster (Ã¶rn: sayfa 3-5, 7, 9)
                page_str = format_page_range(sorted_pages)
                lines.append(f"ğŸ“„ {filename} ({page_str})")
        else:
            lines.append(f"ğŸ“„ {filename}")
    
    return "\n".join(lines) if lines else ""


def format_page_range(pages: List[int]) -> str:
    """
    Sayfa listesini okunabilir formata Ã§evirir.
    Ã–rnek: [1, 2, 3, 5, 7, 8] -> "1-3, 5, 7-8"
    (Sadece sayfa numaralarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r, "sayfa" kelimesi eklenmez)
    """
    if not pages:
        return ""
    
    pages = sorted(set(pages))
    if len(pages) == 1:
        return str(pages[0])
    
    ranges = []
    start = pages[0]
    end = pages[0]
    
    for i in range(1, len(pages)):
        if pages[i] == end + 1:
            end = pages[i]
        else:
            if start == end:
                ranges.append(str(start))
            else:
                ranges.append(f"{start}-{end}")
            start = end = pages[i]
    
    # Son aralÄ±ÄŸÄ± ekle
    if start == end:
        ranges.append(str(start))
    else:
        ranges.append(f"{start}-{end}")
    
    return ", ".join(ranges)


def ensure_dirs():
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(PERSIST_DIR, exist_ok=True)


# --- LLM yardÄ±mcÄ±larÄ± (giriÅŸ uzunluÄŸu gÃ¼venli sÄ±nÄ±rlar) ---
def get_llm() -> HuggingFacePipeline:
    """
    KÃ¼Ã§Ã¼k ve CPU-dostu T5 modeli (otomatik truncation/padding ile).
    """
    from transformers import pipeline as hf_pipeline

    text2text = hf_pipeline(
        "text2text-generation",
        model="google/flan-t5-small",
        truncation=True,
        max_new_tokens=256,
        num_beams=4,
        do_sample=True,
        temperature=0.3,
        top_p=0.9,
        repetition_penalty=1.4,
        no_repeat_ngram_size=6,
        length_penalty=0.2,
        early_stopping=True,
    )
    return HuggingFacePipeline(pipeline=text2text)


def safe_compose_context(docs: List[Document], max_tokens: int = 1024) -> str:
    """
    DokÃ¼man iÃ§eriÄŸini HF tokenizer userÄ±na gerek kalmadan kaba token tahminiyle sÄ±nÄ±rlar.
    """
    approx_ratio = 0.75  # TÃ¼rkÃ§e iÃ§in 1 token ~ 0.75 kelime tahmini
    max_chars = int(max_tokens * approx_ratio * 4)  # ~4 karakter / kelime varsayÄ±mÄ±

    chunks: List[str] = []
    total = 0
    for d in docs:
        part = (d.page_content or "")
        # metni normalize et
        part = " ".join(part.split())
        if not part:
            continue
        take = min(len(part), max(0, max_chars - total))
        if take <= 0:
            break
        chunks.append(part[:take])
        total += take
        if total >= max_chars:
            break
    return "\n\n".join(chunks)


def context_is_relevant(query: str, context: str) -> bool:
    """
    Uygunluk kontrolÃ¼: sorudan Ã§Ä±kan anahtar kelimelerin
    en azÄ±ndan bir kÄ±smÄ± baÄŸlamda geÃ§meli; deÄŸilse reddet.
    CV ve kiÅŸisel bilgi sorularÄ± iÃ§in daha esnek.
    """
    if not context or len(context.strip()) < 20:
        return False
    
    q = " ".join(query.lower().split())
    c = context.lower()
    
    # TÃ¼rkÃ§e stop sÃ¶zcÃ¼klerin kÃ¼Ã§Ã¼k bir alt kÃ¼mesi
    stops = {"ve", "ile", "da", "de", "mi", "bir", "iÃ§in", "ne", "mÄ±", "mÃ¼", "ya", "ama", "veya", "nedir", "nelerdir", "misin", "misiniz", "Ã¶zetler", "Ã¶zet"}
    
    # CV/kiÅŸisel bilgi sorularÄ± iÃ§in Ã¶zel kelimeler
    personal_keywords = {"eÄŸitim", "Ã¼niversite", "okul", "mezun", "bÃ¶lÃ¼m", "bÃ¶lÃ¼mÃ¼", "ad", "isim", "adÄ±m", "kimim", "kim", 
                         "beceri", "yetenek", "proje", "deneyim", "iÅŸ", "Ã§alÄ±ÅŸma", "sertifika", "dil", "iletiÅŸim", "telefon",
                         "cv", "Ã¶zgeÃ§miÅŸ", "bilgi", "detay", "hakkÄ±nda"}
    
    # EÄŸer soru kiÅŸisel bilgi iÃ§eriyorsa ve context varsa, Ã§ok esnek ol
    query_has_personal = any(kw in q for kw in personal_keywords)
    if query_has_personal:
        # CV sorularÄ± iÃ§in Ã§ok toleranslÄ±: context varsa genelde kabul et
        # Sadece Ã§ok kÄ±sa context'leri reddet
        if len(context.strip()) > 30:
            # Ä°ki kelime bile eÅŸleÅŸirse kabul et
            terms = [t for t in q.split() if t not in stops and len(t) > 2]
            if terms:
                hit = sum(1 for t in terms if t in c)
                if hit >= 1:
                    return True
            # HiÃ§ kelime eÅŸleÅŸmese bile, context uzunsa kabul et (Ã§ok toleranslÄ±)
            if len(context.strip()) > 100:
                return True
    
    # Normal kontrol (diÄŸer sorular iÃ§in)
    terms = [t for t in q.split() if t not in stops and len(t) > 3]
    if not terms:
        # EÄŸer soru Ã§ok kÄ±sa veya sadece stop kelimeler iÃ§eriyorsa, context varsa kabul et
        if len(context.strip()) > 50:
            return True
        return False
    hit = sum(1 for t in terms if t in c)
    return hit >= max(1, len(terms) // 3)


def reduce_repetition(text: str) -> str:
    """
    Basit tekrar azaltma: ardÄ±ÅŸÄ±k aynÄ± kelimeleri ve 3-4 kelimelik
    tekrar eden ngramlarÄ± sÄ±kÄ±ÅŸtÄ±rÄ±r.
    """
    import re
    t = re.sub(r"\s+", " ", text).strip()
    t = re.sub(r"\b(\w+)(?:\s+\1){1,}\b", r"\1", t, flags=re.IGNORECASE)
    t = re.sub(r"(\b\w+(?:\s+\w+){2,3}\b)(?:\s+\1){1,}", r"\1", t, flags=re.IGNORECASE)
    return t


def build_prompt(query: str, context: str, prompt_format: str = "kÄ±sa", sources: List[Document] = None) -> str:
    """
    Prompt oluÅŸturur.
    
    Args:
        query: KullanÄ±cÄ± sorusu
        context: Belge baÄŸlamÄ±
        prompt_format: Prompt formatÄ± seÃ§eneÄŸi
            - "kÄ±sa": KÄ±sa ve Ã¶z yanÄ±t
            - "madde": Madde madde liste
            - "Ã¶zet_madde": Ã–nce 1 cÃ¼mle Ã¶zet, sonra 3 madde
            - "Ã¶nce_sonuÃ§": Ã–nce sonuÃ§, sonra gerekÃ§e
        sources: Kaynak dokÃ¼manlar (her belgeden 1 cÃ¼mle kuralÄ± iÃ§in)
    """
    base_instruction = "BaÄŸlamÄ± kullanarak soruya TÃ¼rkÃ§e yanÄ±t ver. BaÄŸlamda soruya doÄŸrudan cevap verecek bilgi yoksa sadece 'Bu belgeden Ã§Ä±karamÄ±yorum.' yaz."
    
    if prompt_format == "kÄ±sa":
        instruction = base_instruction + " KÄ±sa ve net cevap ver.\n\n"
    elif prompt_format == "madde":
        instruction = base_instruction + " YanÄ±tÄ± madde madde ver.\n\n"
    elif prompt_format == "Ã¶zet_madde":
        instruction = base_instruction + " Ã–nce 1 cÃ¼mle Ã¶zet, sonra 3 madde halinde detaylandÄ±r.\n\n"
    elif prompt_format == "Ã¶nce_sonuÃ§":
        instruction = base_instruction + " Ã–nce kÄ±sa sonuÃ§ (1-2 cÃ¼mle), sonra gerekÃ§esini aÃ§Ä±kla.\n\n"
    else:
        instruction = base_instruction + "\n\n"
    
    # Her belgeden en az 1 cÃ¼mle kuralÄ± (Ã§oklu kaynak varsa) - sadece Ã¶zet_madde formatÄ±nda
    if sources and prompt_format == "Ã¶zet_madde":
        unique_sources = set()
        for doc in sources:
            source = doc.metadata.get("source") or "unknown"
            unique_sources.add(os.path.basename(source))
        
        if len(unique_sources) > 1:
            instruction += f"{len(unique_sources)} belgeden bilgi var, her birinden Ã¶rnek ver.\n\n"
    
    return f"{instruction}Soru: {query}\n\nBaÄŸlam:\n{context}\n\nCevap:"

